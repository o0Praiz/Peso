@app.get("/models/{model_name}", dependencies=verify_auth())
async def get_model_info(model_name: str):
    """Get information about a specific model"""
    try:
        model_path = os.path.join(
            peso_integration.ml_engine.models_dir, 
            f"{model_name}_metadata.json"
        )
        
        if not os.path.exists(model_path):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Model {model_name} not found",
            )
        
        with open(model_path, 'r') as f:
            model_info = json.load(f)
            
        return model_info
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting model info: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

# Visualization endpoints

@app.post("/visualizations", dependencies=verify_auth())
async def create_visualization(request: VisualizationRequest):
    """Create a visualization for a dataset"""
    try:
        # Check if dataset exists
        dataset = peso_integration.db.get_dataset(request.dataset_id)
        if not dataset:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Dataset with ID {request.dataset_id} not found",
            )
        
        # Initialize visualization engine
        config = getattr(peso_integration, "config", {})
        viz_dir = config.get("visualization", {}).get("output_dir", "visualizations")
        viz_engine = VisualizationEngine(peso_integration.db, viz_dir)
        
        # Generate visualization based on chart type
        chart_type = request.chart_type.lower()
        result = {}
        
        if chart_type == "basic_stats":
            result = viz_engine.generate_basic_stats_chart(
                request.dataset_id,
                request.columns,
                request.version
            )
        elif chart_type == "distribution":
            result = viz_engine.generate_distribution_plots(
                request.dataset_id,
                request.columns,
                request.version
            )
        elif chart_type == "correlation":
            result = viz_engine.generate_correlation_heatmap(
                request.dataset_id,
                request.columns,
                request.version
            )
        elif chart_type == "scatter_matrix":
            color_by = request.parameters.get("color_by")
            result = viz_engine.generate_scatter_matrix(
                request.dataset_id,
                request.columns,
                color_by,
                request.version
            )
        elif chart_type == "category_comparison":
            if len(request.columns) < 2:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Need category column and value column",
                )
            result = viz_engine.generate_category_comparison(
                request.dataset_id,
                request.columns[0],
                request.columns[1],
                request.version
            )
        elif chart_type == "time_series":
            if len(request.columns) < 2:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Need date column and at least one value column",
                )
            
            # Extract parameters
            group_by = request.parameters.get("group_by")
            interval = request.parameters.get("interval", "D")
            
            result = viz_engine.generate_time_series_chart(#!/usr/bin/env python3
"""
Peso API Module
RESTful API interface for the Peso data warehouse
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import uuid

from fastapi import FastAPI, Depends, HTTPException, Security, status, Request, BackgroundTasks
from fastapi.security import APIKeyHeader
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse, HTMLResponse
from pydantic import BaseModel, Field
import uvicorn

# Import Peso modules
from peso_integration_module import PesoIntegration
from peso_visualization_module import VisualizationEngine

# Initialize logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("peso-api")

# Create API app
app = FastAPI(
    title="Peso API",
    description="API for the Peso Marketing Data Warehouse",
    version="0.6.0",
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Peso Integration
peso_integration = None

# API Key security
api_key_header = APIKeyHeader(name="X-API-Key")

# Pydantic models for request/response validation
class DatasetCreate(BaseModel):
    name: str = Field(..., description="Name of the dataset")
    data_type: str = Field(..., description="Type of dataset (human, non-human, mixed)")
    synthetic_count: int = Field(100, description="Number of synthetic records to generate")

class DatasetEnrich(BaseModel):
    dataset_id: int = Field(..., description="ID of the dataset to enrich")
    ai_tool: str = Field("openai", description="AI tool to use for enrichment")

class MLModelTrain(BaseModel):
    dataset_id: int = Field(..., description="ID of the dataset")
    target_column: str = Field(..., description="Column to predict")
    model_type: str = Field("classifier", description="Type of model (classifier/regressor)")
    features: Optional[List[str]] = Field(None, description="List of feature columns")

class PredictionRequest(BaseModel):
    model_name: str = Field(..., description="Name of the trained model")
    data: List[Dict[str, Any]] = Field(..., description="Data to make predictions on")

class QueryRequest(BaseModel):
    dataset_id: Optional[int] = Field(None, description="Dataset ID to query")
    filters: Dict[str, Any] = Field({}, description="Filter criteria")
    group_by: Optional[str] = Field(None, description="Field to group by")
    aggregations: Optional[List[Dict[str, str]]] = Field(None, description="Aggregation specifications")

class VisualizationRequest(BaseModel):
    dataset_id: int = Field(..., description="ID of the dataset")
    chart_type: str = Field(..., description="Type of chart to generate")
    columns: List[str] = Field(..., description="Columns to include in visualization")
    parameters: Dict[str, Any] = Field({}, description="Additional parameters for visualization")
    version: Optional[int] = Field(None, description="Dataset version")

class DashboardRequest(BaseModel):
    dataset_id: int = Field(..., description="ID of the dataset")
    version: Optional[int] = Field(None, description="Dataset version")

def get_api_key(api_key: str = Security(api_key_header)):
    """Validate API key"""
    config = getattr(peso_integration, "config", {})
    api_keys = config.get("api", {}).get("api_keys", [])
    
    if not api_keys or api_key in api_keys:
        return api_key
    
    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Invalid API key",
    )

def verify_auth():
    """Check if authentication is required"""
    config = getattr(peso_integration, "config", {})
    require_auth = config.get("api", {}).get("require_auth", True)
    
    if require_auth:
        return [Depends(get_api_key)]
    return []

@app.on_event("startup")
async def startup_event():
    """Initialize Peso integration on startup"""
    global peso_integration
    
    try:
        # Initialize integration
        peso_integration = PesoIntegration()
        
        # Create visualization output directory if it doesn't exist
        config = getattr(peso_integration, "config", {})
        viz_dir = config.get("visualization", {}).get("output_dir", "visualizations")
        if not os.path.exists(viz_dir):
            os.makedirs(viz_dir)
            
        logger.info("Peso API initialized successfully")
    except Exception as e:
        logger.error(f"Error initializing Peso API: {e}")
        raise

@app.get("/", dependencies=verify_auth())
async def root():
    """API root endpoint"""
    return {
        "name": "Peso API",
        "version": "0.6.0",
        "status": "online",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """Health check endpoint (no auth required)"""
    if peso_integration is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Service not initialized",
        )
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/datasets", dependencies=verify_auth())
async def list_datasets(name: Optional[str] = None, type: Optional[str] = None):
    """List all datasets with optional filtering"""
    filters = {}
    
    if name:
        filters["name_contains"] = name
    
    if type:
        filters["type"] = type
    
    try:
        datasets = peso_integration.query_engine.query_datasets(filters)
        return {
            "count": len(datasets),
            "datasets": datasets
        }
    except Exception as e:
        logger.error(f"Error listing datasets: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

@app.get("/datasets/{dataset_id}", dependencies=verify_auth())
async def get_dataset(dataset_id: int, version: Optional[int] = None):
    """Get a specific dataset"""
    try:
        dataset = peso_integration.db.get_dataset(dataset_id, version)
        
        if not dataset:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Dataset with ID {dataset_id} not found",
            )
        
        return dataset
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting dataset: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

@app.post("/datasets", dependencies=verify_auth())
async def create_dataset(dataset: DatasetCreate):
    """Create a new dataset"""
    try:
        dataset_id = peso_integration.collect_and_process_data(
            name=dataset.name,
            data_type=dataset.data_type,
            synthetic_count=dataset.synthetic_count
        )
        
        if dataset_id < 0:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to create dataset",
            )
        
        return {
            "dataset_id": dataset_id,
            "message": f"Created dataset {dataset.name} with {dataset.synthetic_count} records"
        }
    except Exception as e:
        logger.error(f"Error creating dataset: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

@app.post("/datasets/enrich", dependencies=verify_auth())
async def enrich_dataset(request: DatasetEnrich, background_tasks: BackgroundTasks):
    """Enrich a dataset using AI"""
    # Check if dataset exists
    dataset = peso_integration.db.get_dataset(request.dataset_id)
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Dataset with ID {request.dataset_id} not found",
        )
    
    # Run enrichment in background task
    task_id = str(uuid.uuid4())
    
    background_tasks.add_task(
        enrich_dataset_task,
        request.dataset_id,
        request.ai_tool,
        task_id
    )
    
    return {
        "task_id": task_id,
        "message": f"Started enrichment of dataset {request.dataset_id} using {request.ai_tool}",
        "status": "processing"
    }

async def enrich_dataset_task(dataset_id: int, ai_tool: str, task_id: str):
    """Background task for dataset enrichment"""
    try:
        success = peso_integration.enrich_dataset_with_ai(dataset_id, ai_tool)
        
        # In a real implementation, store task status for later retrieval
        logger.info(f"Enrichment task {task_id} completed with status: {success}")
    except Exception as e:
        logger.error(f"Error in enrichment task {task_id}: {e}")

@app.get("/datasets/{dataset_id}/analyze", dependencies=verify_auth())
async def analyze_dataset(dataset_id: int, background_tasks: BackgroundTasks):
    """Analyze a dataset"""
    # Check if dataset exists
    dataset = peso_integration.db.get_dataset(dataset_id)
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Dataset with ID {dataset_id} not found",
        )
    
    # Run analysis in background task
    task_id = str(uuid.uuid4())
    
    background_tasks.add_task(
        analyze_dataset_task,
        dataset_id,
        task_id
    )
    
    return {
        "task_id": task_id,
        "message": f"Started analysis of dataset {dataset_id}",
        "status": "processing"
    }

async def analyze_dataset_task(dataset_id: int, task_id: str):
    """Background task for dataset analysis"""
    try:
        analysis = peso_integration.analyze_dataset(dataset_id)
        
        # In a real implementation, store results for later retrieval
        logger.info(f"Analysis task {task_id} completed successfully")
    except Exception as e:
        logger.error(f"Error in analysis task {task_id}: {e}")

@app.post("/ml/train", dependencies=verify_auth())
async def train_model(request: MLModelTrain, background_tasks: BackgroundTasks):
    """Train a machine learning model"""
    # Check if dataset exists
    dataset = peso_integration.db.get_dataset(request.dataset_id)
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Dataset with ID {request.dataset_id} not found",
        )
    
    # Run training in background task
    task_id = str(uuid.uuid4())
    
    background_tasks.add_task(
        train_model_task,
        request.dataset_id,
        request.target_column,
        request.model_type,
        request.features,
        task_id
    )
    
    return {
        "task_id": task_id,
        "message": f"Started training {request.model_type} model for dataset {request.dataset_id}",
        "status": "processing"
    }

async def train_model_task(dataset_id: int, target_column: str, 
                          model_type: str, features: List[str], task_id: str):
    """Background task for model training"""
    try:
        model_info = peso_integration.train_ml_model(
            dataset_id, 
            target_column,
            model_type
        )
        
        # In a real implementation, store results for later retrieval
        logger.info(f"Training task {task_id} completed successfully: {model_info.get('model_name')}")
    except Exception as e:
        logger.error(f"Error in training task {task_id}: {e}")

@app.post("/ml/predict", dependencies=verify_auth())
async def predict(request: PredictionRequest):
    """Make predictions with a trained model"""
    try:
        predictions = peso_integration.make_predictions(
            request.model_name,
            request.data
        )
        
        if "error" in predictions:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=predictions["error"],
            )
        
        return predictions
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error making predictions: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

@app.post("/query", dependencies=verify_auth())
async def query_data(request: QueryRequest):
    """Query data with flexible criteria"""
    try:
        if request.dataset_id:
            # Query within a specific dataset
            query_spec = {
                "dataset_id": request.dataset_id,
                "content_search": request.filters
            }
            
            if request.group_by:
                query_spec["group_by"] = request.group_by
                
            if request.aggregations:
                query_spec["aggregations"] = request.aggregations
                
            results = peso_integration.query_engine.advanced_query(query_spec)
        else:
            # Query datasets
            results = {
                "datasets": peso_integration.query_engine.query_datasets(request.filters),
                "count": 0
            }
            results["count"] = len(results["datasets"])
        
        return results
    except Exception as e:
        logger.error(f"Error querying data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

@app.get("/models", dependencies=verify_auth())
async def list_models():
    """List all trained models"""
    try:
        models = peso_integration.ml_engine.list_models()
        return {
            "count": len(models),
            "models": models
        }
    except Exception as e:
        logger.error(f"Error listing models: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

@app.get("/models/{model_name}", dependencies=verify_auth())
async def get_model_info(model_name: str):
    """Get information about a specific model"""
    try:
        model_path = os.path.join(
            peso_integration.ml_engine.models_dir, 
            f"{model_name}_metadata.json"
        )
        
        if not os.path.exists(model_path):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Model {model_name} not found",
            )
        
        with open(model_path, 'r') as f:
            model_info = json.load(f)
            
        return model_info
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting model info: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

def run_api_server(host: str = "127.0.0.1", port: int = 8000):
    """Run the API server"""
    uvicorn.run(app, host=host, port=port)

if __name__ == "__main__":
    # Get host and port from environment or use defaults
    host = os.environ.get("PESO_API_HOST", "127.0.0.1")
    port = int(os.environ.get("PESO_API_PORT", "8000"))
    
    # Run the server
    run_api_server(host, port)